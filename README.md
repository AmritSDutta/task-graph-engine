# FlowCheck

# Using Orchestratorâ€“Worker Architecture

## Overview

FlowCheck implements a parallel evaluation pipeline for decision automation using **LangGraph** with an **orchestratorâ€“worker fanâ€‘out/fanâ€‘in pattern**. The system:

1. Extracts and summarizes input context
2. Dispatches parallel decision evaluators
3. Aggregates all outputs into a unified final report

This design enables scalable execution while ensuring deterministic aggregation of results.

---

## Architectural Components

### âœ… Summarizer (Global Context Builder)

* Node: `summarizer`
* Model: **gemini-2.5-flash-lite**
* Responsibilities:

  * interpret raw issue input
  * generate structured `issue`
  * produce `sub_issues_decision: list[DecisionOutput]`

### âœ… Fanâ€‘Out Executor (Orchestrator)

* Implemented as **conditional edge**, not a node
* Function: `assign_workers`
* Generates:

  ```python
  [Send("subtask", payload) ...]
  ```
* Each dispatched branch receives isolated `sub_issue`

### âœ… Worker Nodes (Parallel Evaluation)

* Node: `subtask`
* Model: **gptâ€‘5â€‘nano**
* Execution via:

  ```python
  Runner.run(starting_agent, input, context)
  ```
* Returns:

  ```python
  {"completed_sub_issues_decision": [DecisionOutput]}
  ```

### âœ… Fanâ€‘In Aggregator

* Node: `combiner`
* Input merged automatically because:

  ```python
  completed_sub_issues_decision: Annotated[list[DecisionOutput], operator.add]
  ```
* Produces:

  ```python
  final_report: CombinedPlan
  ```

---

## State Definition

```python
class State(TypedDict):
    retry_count: Annotated[int, add]
    messages: Annotated[list[BaseMessage], add_messages]
    issue: str
    sub_issues_decision: list[DecisionOutput]
    sub_issue: NotRequired[DecisionOutput]
    completed_sub_issues_decision: Annotated[list[DecisionOutput], operator.add]
    final_report: CombinedPlan
```

### Why this matters

* `operator.add` enables list concatenation during fanâ€‘in
* `sub_issue` is optional because it only exists inside worker branches
* messages and retry_count remain compatible with LangGraph execution

---

## Execution Flow

```
START
  â†“
summarizer
  â†“
assign_workers  (conditional edge)
  â”œâ”€ Send â†’ subtask (worker 1)
  â”œâ”€ Send â†’ subtask (worker 2)
  â”œâ”€ Send â†’ subtask (worker 3)
  â€¦
  â†“ (after all workers complete)
combiner
  â†“
END
```

---

## Key Rules and Guarantees

âœ… Fanâ€‘out must return Send(), not dict
âœ… Fanâ€‘out must not be registered as a node
âœ… Worker return values must be dicts
âœ… Worker outputs must be lists
âœ… Shared state must be passed into Send payload
âœ… Dynamic instructions must escape braces if using fâ€‘strings
âœ… Null fields like `notes` must be normalized

---
## ğŸ” Example: Dynamic Evaluator Prompt (run_connectivity_diagnostics)

Each evaluator receives a dynamically constructed prompt based on its `decision_id`.  
Below is an example of the **actual prompt** used for the `run_connectivity_diagnostics` evaluator.

### **Evaluator Prompt (Auto-Generated)**

```
as a agent of run_connectivity_diagnostics. 
You are an automated decision evaluator.

Input:
- decision_id: run_connectivity_diagnostics
- context: unstructured text containing events, logs, symptoms, actions, or user reports.

Task:
1. Read and interpret the context.
2. Based solely on the meaning of the decision_id, determine if action is required:
   - Triggered by network failures, unreachable services, or packet loss indications.
3. Return:
   - decision: true if action is warranted, false otherwise
   - confidence: 0.0â€“1.0 expressing certainty
   - model: name of the model producing the output
   - notes: concise reasoning (optional) , must be maximum 10 words.
   - latency_ms: leave empty

Output JSON strictly in the following structure:

{
  "decision_id": "run_connectivity_diagnostics",
  "decision": true or false,
  "confidence": 0.0,
  "model": "gpt-5-nano",
  "notes": "short rationale",
  "latency_ms": null
}
optimize output token usage without compromising on quality of output.
Help them with their questions.
```

---

### **Sample Output**

```json
{
  "decision_id": "run_connectivity_diagnostics",
  "decision": true,
  "confidence": 0.81,
  "model": "gpt-5-nano",
  "notes": "Latency spikes and packet loss reported",
  "latency_ms": null
}
```
### Notes

- Every evaluator follows the same structure; only decision_id, decision logic description, and sample schema differ.
- Prompts remain short to minimize cost while preserving clarity.
---

The executor uses these outputs to assemble the final CombinedPlan.
---

## Model Selection Rationale

| Component       | Model                  | Reason                               |
| --------------- | ---------------------- | ------------------------------------ |
| summarizer      | **gptâ€‘2.5â€‘flashâ€‘lite** | inexpensive global contextualization |
| subtask workers | **gptâ€‘5â€‘nano**         | fast, structured, parallelizable     |
| final combiner  | inherits context       | purely deterministic merging         |

This yields costâ€‘efficient scaling because heavy reasoning doesnâ€™t run per worker.

---

## Suggested Enhancements

âœ… concurrency limiter (e.g., max 3 workers)
âœ… telemetry: latency, decision rate, disagreement counts
âœ… cost attribution per decision_id
âœ… retry policy only at worker level

---

## When to Use This Architecture

Use it if you need:
âœ… independent evaluations per decision type
âœ… consistent aggregation
âœ… heterogeneous model assignment
âœ… parallelism with deterministic merge semantics

Do **not** use if:
âŒ decisions depend on each other
âŒ ordering impacts evaluation

---
# FlowCheck UI (Streamlit Client)

A lightweight UI for interacting with the FlowCheck LangGraph deployment over REST. It supports large incident input, run execution, polling run status, and displaying the final `final_report` from thread state.

## Requirements
```bash
pip install streamlit requests
```

## Configuration
Edit at top of `app.py`:
```python
DEPLOYMENT_URL = "http://localhost:2024"
ASSISTANT_ID = "agent"
```

## Run
```bash
python -m streamlit run app.py
```

Open in browser:
```
http://localhost:8501
```

## Features
- Large text issue input
- Sends request to LangGraph deployment
- Shows compact â€œrunningâ€ status
- Fetches thread state after success
- Displays formatted `final_report`

## Troubleshooting
| Issue | Fix |
|-------|-----|
| 422 on thread create | Add `json={}` body |
| No final report | Read from thread, not run |
| Connection failure | Check deployment URL & server |

## Optional Enhancements
- Show node transitions
- Export report file
- Use new thread per run
- Add auth headers




## License

Internal architectural documentation for FlowCheck.
